{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REM Memory Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Explanation\n",
    "\n",
    "Model is used to explain performance in Episodic Memory Task. \n",
    "\n",
    "The task goes as follows: \n",
    "- Subject is given a list of study items (e.g.) words and has to learn them. \n",
    "- After studying a filler task is performed (e.g. completing a puzzle.) \n",
    "- In the test phase, the subject is presented with items, that were either on the study set or are entirely new. \n",
    "- The subject has to respond with \"old\" if the item has been encountered the study phase, or \"new\" if the item has not been encountered in the study phase. \n",
    "\n",
    "Learned words are called \"targets\" and words which where not learned are called \"distractors\". \n",
    "\n",
    "For simplicity we focus on two cases:\n",
    "- $hit:$ target is presented and subject correctly responds with 'old'\n",
    "- $false-alarm:$ distractor is presented and subject incorrectly responds with 'old'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation\n",
    "\n",
    "REM is a global memory which means that the recognition response of a single memory probe is based on a calculation of familiarity between the probe item and all other items of the study list. Each Item is composed of a list of feature or feature vector with w dimensions. This vector is called trace. The psychological interpretation of this trace is ... \n",
    "\n",
    "### Feature Generation \n",
    "\n",
    "Mathematically speaking each of the features follow a geometric distribution:\n",
    "\n",
    "$P(K = k) = (1-g)^{k-1} *g$ with $k \\; \\epsilon \\; [1,2,... \\infty] $ and $g \\; \\epsilon \\; (0,1)$ \n",
    "\n",
    "where g is the so called 'environmental base rate' which is high for high-frequency words and lower for low-frequency words. Because for large values of g as in high-frequency, the variance for different values for a feature K will be lower, thus they will have more common features, which makes them less distinct.\n",
    "\n",
    "Example:\n",
    "\n",
    "- q = 0.35 -> [ 1  2  5  8  6  2  2  1  2 10  4  1  1  1  1  5  3  8  2  1] indicating that a high-frequency word is less distinct in their features\n",
    "- q = 0.1 -> [ 2 37  9 29 11 11 19 13  4 15  9  6  7 30 13  2 13  5  8  2] indicating that a low-frequency word is more distinct in their features\n",
    "\n",
    "\n",
    "### 1) Study of Words (storage)\n",
    "\n",
    "During study the features of a given word are copied to a memory trace. This copying process is error-prone and incomplete. Initially the representation of a feature vector for a particular item only contains zeros. Then the copying process fills the vector with values according to the following probabilities \n",
    "\n",
    "- $u:$ A feature is copied with this probabilty and not copied with the probability $1-u$ meaning the entry will remain zero\n",
    "- $c:$ A feature when copied, can still be copied incorrectly with this probability. In the opposite case $1-c$ a feature will be copied randomly by drawing again from the geometric distribution using base rate $g$. \n",
    "\n",
    "\n",
    "The copying process consists of two steps:\n",
    "1. A feature is copied into trace with probability u and will remain empty with probability 1-u\n",
    "2. A feature is copied correctly with probability c and incorrectly 1-c by resampling\n",
    "\n",
    "This is repeated for each feature of all studies items. This can be represented in a wxn matrix called the episodic matrix.\n",
    "\n",
    "\n",
    "### 2) Recall of Words (retrieval)\n",
    "\n",
    "During retrieval we are presented with a test-item (probe), that is then compared to each trace in the episodic matrix. We need to calculate the following to quantities:\n",
    "\n",
    "- $n_{jq}$ which tells us the number of non-zero mismatching features in the trace j \n",
    "- $n_{ijm}$ which tells us the number of non-zero matching type features in the j-th trace with the value of i\n",
    "\n",
    "Then similarity $ \\lambda _{j}$ is calculated in the following expression:\n",
    "\n",
    "$ \\lambda _{j} = (1-c)^{n_{jq}} \\prod_{i = 1}^{\\infty} \\left[  \\frac{c+(1-c)g(1-g)^{i-1}}{g(1-g)^{i-1}} \\right]^{n_{ijm}} $ \n",
    "\n",
    "\n",
    "The overall familarity with a given probe item can then be written as its expectation over the similarities with each trace item \n",
    "$ \\phi = \\frac{1}{n} \\sum_{j = 1}^{n} \\lambda _{j} $\n",
    "\n",
    "The familiarity $\\phi$ then is actually the expectation of a likelihood ratio: \n",
    "the probability that the probe is a target divided by the probability that the probe is a distractor. The Bayesian Decision rules then states to accept such probes that $ \\phi > 1$ meaning an \"old\"-response is given. In the opposite case a \"new\" response is given.\n",
    "\n",
    "We now do this for every probe item and count the the number of 'old' responses, which we then use to assess wether those were hits or false alarms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "Our goal is now to infer the parameters g,u and c for a single simulated subject in a recognition memory experiment with two list-length (n either 10 or 20) conditions. The test lists consist of the entire previously list plus 10 or 20 distactor items for the short and long list conditions. \n",
    "\n",
    "The following 3 stages will be required:\n",
    "\n",
    "1. Select values for g, u , c and generate a stimulus set using parameter g. \n",
    "2. Fill the Episodic Matrix during the study phase using the parameters g,u and c.\n",
    "3. Finally complete the test-phase by using the same parameters g, u in the equation $ \\lambda _{j} = (1-c)^{n_{jq}} \\prod_{i = 1}^{\\infty} \\left[  \\frac{c+(1-c)g(1-g)^{i-1}}{g(1-g)^{i-1}} \\right]^{n_{ijm}} $ \n",
    "\n",
    "We set  $g,u,c \\sim Beta(1,1)$\n",
    "\n",
    "These parameters are used over each condition, the only quantity that changes is the is the size of the study list, which is specific to the conditiion\n",
    "\n",
    "\n",
    "## Distance Function:\n",
    "\n",
    "In each recognition memory experiment we observed the number of hits and false alarms across the different conditions.\n",
    "For a condition j we calculate the following:\n",
    "\n",
    "$ Y_{j,HIT} \\sim Bin(n_{j, OLD} , p_{HIT} )$ and $Y_{j, FA} \\sim Bin(n_{j, NEW} , p_{FA} )$. \n",
    "\n",
    "The likelihood of the joint event $(Y_{j, HIT}, Y_{j, FA} ) $ -> the product of the two probabilities\n",
    "\n",
    "\n",
    "The distance function then is described as \n",
    "\n",
    "$\\rho(X,Y) = \\frac{1}{2C} \\left[ \\sum_{j = 1}^{C} \\left| \\; (x_{j,FA} - Y_{j,FA}) / N_{new} \\; \\right|\n",
    "+ \\sum_{j = 1}^{C} \\left| \\; (x_{j,HIT} - Y_{j,HIT}) / N_{old} \\right| \\right] $ with a maximum value of one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import pyabc.prior\n",
    "from pyabc.plots import plot_marginals\n",
    "from pyabc.examples import rem_memory\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_fa_rate(results): \n",
    "    summarized = np.zeros((len(results), 4))\n",
    "    for i in range(len(results)):\n",
    "        result = results[i]\n",
    "        \n",
    "        #currently only supports test lists that are twice the size of study\n",
    "        n_study = int(len(result) / 2)\n",
    "        n_test = len(result)\n",
    "    \n",
    "        n_hits = np.sum(result[0:n_study]) \n",
    "        n_fa = np.sum(result[n_study:])\n",
    "        \n",
    "        summarized[i,:] = [n_hits,n_fa, n_study, (n_test - n_study)]\n",
    "    \n",
    "    return summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance function then is described as \n",
    "\n",
    "$\\rho(X,Y) = \\frac{1}{2C} \\left[ \\sum_{j = 1}^{C} \\left| \\; (x_{j,FA} - Y_{j,FA}) / N_{new} \\; \\right|\n",
    "+ \\sum_{j = 1}^{C} \\left| \\; (x_{j,HIT} - Y_{j,HIT}) / N_{old} \\right| \\right] $ with a maximum value of one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_function(x, y):\n",
    "    hit = 0\n",
    "    fas = 0 \n",
    "    conditions = int(len(x) / 4)\n",
    "    for i in range(conditions):     \n",
    "        hit += np.abs( (x[i*4+0]-y[i*4+0]) / y[i*4+2]) \n",
    "        fas += np.abs( (x[i*4+1]-y[i*4+1])  / y[i*4+3])    \n",
    "    return 1/(2*conditions) * (fas+hit)\n",
    "\n",
    "def distance_function2(x, y):\n",
    "    hit = 0\n",
    "    fas = 0 \n",
    "    conditions = int(len(x))\n",
    "    for i in range(conditions):     \n",
    "        hit += np.abs( (x[i,0]-y[i,0]) / y[i,2]) \n",
    "        fas += np.abs( (x[i,1]-y[i,1])  / y[i,3])    \n",
    "    return 1/(2*conditions) * (fas+hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0.6\n",
    "u = 0.335\n",
    "c = 0.7\n",
    "\n",
    "simulator = lambda u, g, c: rem_memory.simulator(u, g, c, w=9, conditions=[10,20])\n",
    "\n",
    "\n",
    "y0 = simulator(u,g,c)\n",
    "\n",
    "#we set g = 0.6, u = 0.335, and c = 0.7.\n",
    "\n",
    "g_prior = pyabc.Prior(\"beta\", 1, 1, name=\"g\")\n",
    "u_prior = pyabc.Prior(\"beta\", 1, 1, name=\"u\")\n",
    "c_prior = pyabc.Prior(\"beta\", 1, 1, name=\"c\")\n",
    "\n",
    "#y = simulator(g_prior.sample(),u_prior.sample(), c_prior.sample()) \n",
    "\n",
    "#y0_summary = calculate_hit_fa_rate(y0)\n",
    "#y_summary = calculate_hit_fa_rate(y)\n",
    "\n",
    "#distance_function(y0_summary,y_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = simulator(u,g,c)\n",
    "y = simulator(u_prior.sample(),g_prior.sample(),c_prior.sample())\n",
    "\n",
    "x_summary = calculate_hit_fa_rate(x)\n",
    "y_summary = calculate_hit_fa_rate(y)\n",
    "\n",
    "distance_function2(x_summary,y_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rej = pyabc.RejectionSampler(priors=[u_prior,g_prior,c_prior], simulator=simulator, \n",
    "                             summaries=[calculate_hit_fa_rate], distance = distance_function,\n",
    "                             observation=y0)\n",
    "smc = pyabc.SMCSampler(priors=[u_prior,g_prior,c_prior], simulator=simulator, \n",
    "                             summaries=[calculate_hit_fa_rate], distance = distance_function,\n",
    "                             observation=y0, verbosity = True)\n",
    "mcmc = pyabc.MCMCSampler(priors=[u_prior,g_prior,c_prior], simulator=simulator, \n",
    "                             summaries=[calculate_hit_fa_rate], distance = distance_function,                   \n",
    "                         observation=y0, verbosity = True)\n",
    "abcde = pyabc.ABCDESampler(priors=[u_prior,g_prior,c_prior], simulator=simulator, \n",
    "                             summaries=[calculate_hit_fa_rate], distance = distance_function,                   \n",
    "                         observation=y0, verbosity = True)\n",
    "bolfi = pyabc.BOLFI(priors=[u_prior,g_prior,c_prior], simulator=simulator, \n",
    "                             summaries=[calculate_hit_fa_rate], distance = distance_function,                   \n",
    "                         observation=y0, verbosity = True, domain=[(0,1), (0,1), (0,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rej.sample(nr_samples = 1000, threshold = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smc.sample(nr_samples=1000, thresholds=[0.5, 0.2, 0.15, 0.1, 0.08, 0.06,0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.sample(nr_samples=1000, threshold = 0.03, step_size = [0.05,0.05,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcde_samp.sample(nr_samples=1000, nr_groups = 10, nr_iter = 200, burn_in = 100, alpha = 0.1, beta = 0.1, kappa = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolfi.sample(nr_samples=10000, threshold=0.01, n_chains=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marginals(rej, kde=True, normed=True)\n",
    "plot_marginals(mcmc, kde=True, normed=True)\n",
    "plot_marginals(smc, kde=True, normed=True)\n",
    "plot_marginals(abcde, kde=True, normed=True)\n",
    "plot_marginals(bolfi, kde =True, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging of individual functions  \n",
    "#Sanity Check comparing our math to (Shiffrin 1997 Figure 1)\n",
    "c = 0.7\n",
    "gh = 0.45\n",
    "g = 0.4\n",
    "u = 0.5\n",
    "\n",
    "study_target1 = np.array([6,1,1,3])\n",
    "study_target2 = np.array([3,2,1,1])\n",
    "study_list = [study_target1, study_target2]\n",
    "\n",
    "episodic_image1 = np.array([0,1,0,3])\n",
    "episodic_image2 = np.array([2,2,1,0])\n",
    "episodic_matrix = [episodic_image1,episodic_image2]\n",
    "\n",
    "probe_dist = np.array([2,3,4,3])\n",
    "probe_target = np.array([6,1,1,3])\n",
    "probe_list = [probe_dist,probe_target]\n",
    "\n",
    "\n",
    "n_test = len(probe_list)\n",
    "results = np.zeros(n_test)\n",
    "\n",
    "for i in range(n_test):\n",
    "        total = 0 \n",
    "        for j in range(len(episodic_matrix)):\n",
    "            total += calculate_similarity(probe_list[i],episodic_matrix[j],u,g,c)\n",
    "   \n",
    "        phi = 1/(len(probe_list)) * total\n",
    "    \n",
    "        if phi > 1: \n",
    "            results[i] = 1 \n",
    "        else:\n",
    "            results[i] = 0     \n",
    "        print(\"----\")  \n",
    "        print('phi:', phi)\n",
    "        print('accept:' , results[i])\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ABC)",
   "language": "python",
   "name": "pyabc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

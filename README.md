# Approximate Bayesian Computation
In recent years, the developments in sampling algorithms for Bayesian inference along with the advances of computational resources have enabled researchers to efficiently compute approximations of posterior distributions in models that were previously not viable due to their intractability in closed-form. However, these methods still rely on the ability to calculate the likelihood, i.e. the probability of the data given the model parameters. For many domains and realistic scenarios, this likelihood function is intractable or simply unknown, but one can specify a (stochastic) simulator function that generates data from the parameters. This is where a class of methods called Approximate Bayesian Computation (ABC), which bypass calculating the likelihood, comes into play.
Our goal during this project is to provide a comprehensive overview of ABC methods and their limitations, including sampling based, synthetic likelihood and Regression ABC approaches. We will implement the most commonly used algorithms in a Python package in order to overcome shortcomings of existing packages, which are often too domain-specific or lack important features such as support of multidimensional priors. Finally, our aim is to use our implementation to illustrate possible applications of ABC methods in Cognitive Science.
